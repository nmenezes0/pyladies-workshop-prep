title,abstract,description,prerequisites
"I want to deploy my Flask app on Kubernetes, what are my options?","After creating a great web app using Python such as with flask, the next hurdle to production is how to make it available to users and operate it. And not just your app, but also ingress, the database, observability and the list goes on. We will go through your options for simplifying the operations of your web app using open source tooling. This will include using k8s directly with helm charts, PaaS using fly.io and new tooling developed by Canonical using juju. By the end of the talk you will have seen th",,"Some web application development experience, can be beginner"
"Before you Scale, let's talk SOLID - Significance of Solid Principles as a Prerequisite for Scaling Software Systems","In this session we will discuss the importance of SOLID principles as a prerequisite for scaling Software systems. We will talk about what software scaling is, why scaling in open-source projects matters and the challenges project maintainers face while scaling projects. We will also look into what SOLID principles are and why each of the principles matters for software scaling using Python and Django. Attendees will gain knowledge on how to apply SOLID principles using Python and Django.","The session will begin with A brief introduction to Software Scaling,  why Scaling matters and challenges of scaling. an introduction to the SOLID principles and why each of the principles matters for scaling, that is; Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, and Dependency Inversion with code illustration.",Intermediate or Expert
"Coding Aesthetics: PEP 8, Existing Conventions, and Beyond","Coding aesthetics, in this context, refers to how code is written. It is essential that programmers also pay attention to the aesthetics and not just the functionality the code aims to achieve. This talk explores several ways to make Python code aesthetically pleasing, such as code refactoring, using static code analysis tools like PyLint to check compliance with PEP8 guidelines, and applying syntactic sugar. In addition, we will discuss the limitations of PEP8 and how we can make more pragmatic choices.","The talk will start with a few questions, and also with Donald Knuth's quote: ""Programming is the art of telling another human being what one wants the computer to do.""",Just Basic skills and experience with Python programming language should be fine.
Code review the right way,"Code review is central part of everyday developer job. The motivation to create this talk was a quote:
“The most important superpower of a developer is complaining about everybody else's code”. In this talk I’ll explain my approach to better code review.","Sometimes it’s hard to convince a colleague about change or don’t change some lines of code, in my talk I would like to cover some best practices from my software engineering experience about efficient and honest code review. How to create culture of perfect code review. How to apply automatic tools to improve code review routine of repetitive comments or suggestions. How to write/or reuse codign style guides for you team to reduce time of arguing about naming conventions and different styles.
What’s need to be automated and what’s need to be not automated during code review. The key role of patterns which can be reusable to not confuse colleagues.",just basic ideas about code review
Sync or async? Feel the magic of coroutines and the event loop in Django,Struggling with slow I/O in your Django apps? Want to maximize server resources? This talk explores asynchronous Python and its impact on Django.,Are you struggling with slow I/O operations in your Django applications? Do you want to unlock the full potential of your server's resources? This talk dives into the world of asynchronous programming in Python and explores how it can work for your Django applications!,"At least knowing how to code in Python. Knowledge of parallel or concurrent programming might help, but this will still be explained in the beginning."
Architecture as Code (AaC) with Python,"Architecture as Code (AaC) was born for prototyping a new system architecture design without any design tools. Available tools currently support on-premise and main major providers including AWS, Azure, and GCP cloud platforms.","Have you already learned the benefits of the Infrastructure as Code (IaC) process? Not bad, now it’s time for the Architecture as Code (AaC) process. No worries, Python code only, no more JSON or YAML. AaC was born for prototyping a new system architecture design without any design tools. Available tools currently support on-premise and main major providers including AWS, Azure, and GCP cloud platforms. In addition, AaC allows you to track the architecture diagram changes in any version control system.",There are no prerequisites for attending this talk; everyone is welcome regardless of prior knowledge.
The art of yield,"Can you imagine a python project without any return?
Is it overhead or memory saving? Is it complicated or would it reduce complexity? Is it testable or a horror for unittesting?","The general idea of this talk is: to effectively use generators in code, we need to change our programming style, and it's not too easy, but it's possible. During my talk I will convert functions and methods from the project into generators, and we will see what is effective, what is not, and where it is still better to use retur",knowlege about lazy evaluations in python
Using Trusted Publishing to Ansible release,"""Trusted publishing"" is the term for using the OpenID Connect (OIDC) standard in the Python Ecosystem to release on PyPI.  In this talk will go though the usage of trusted publishing in any Python project and how it helped Ansible project to open up release management to the community. This talk is a deep dive explanation of release practicalities of releasing Ansible using trusted publishing.","""Trusted publishing"" is the  the way of  exchanging short-lived identity tokens between a trusted third-party service and PyPI. This key feature in PyPI empowers the project maintainers to make releases via automated environments directly . This helps us to get rid of the use of manually generated API tokens. This talk will dig deeper in the practical aspects and impact of moving manual  release process to automated release via github actions and trusted publishing. The  talk will describe the trusted publishing from the view of a Release Manager of a critical project like Ansible.",Not appliacble
Beyond the GIL: Python's Evolution and Future Directions,"Explore the transformative journey of Python's Global Interpreter Lock (GIL). Delve into the GIL's origins, its role in Python's growth, and its challenges for multicore processing in development. Let's discover the implications of its experimental removal in Python 3.13.1, and how this shift might redefine concurrency, performance, and the future landscape of Python applications across various domains",,Python basics
Python in 3D computer graphics,"Python has emerged as a versatile tool for 3D computer graphics, offering powerful capabilities in modeling, animation, and simulation. This presentation explores the application of Python in creating dynamic and visually engaging 3D graphics using Blender. The session will showcase practical examples that demonstrate Python's potential in various aspects of 3D graphics","Explore techniques for programmatically creating and manipulating 3D models in Blender using Python scripting.
Illustrate how Python can be utilized to animate statistical data in a 3D environment.
Present a Python implementation of the Game of Life cellular automaton within Blender.
Showcase Python scripts for creating randomized animations in Blender.",basics of Python
"Build, Deploy, Monetize: The Future of the Developer Economy","The Creator Developer Economy offers developers the chance to turn their skills into a passive income stream. In this talk, I’ll explore how developers can leverage Apify's tools to build, deploy, and monetize web scraping solutions. From using Crawlee for Python to create efficient scrapers to publishing and earning.","The Creator Developer Economy offers developers the chance to turn their skills into a passive income stream. In this talk, I’ll explore how developers can leverage Apify's tools to build, deploy, and monetize web scraping solutions. From using Crawlee for Python to create efficient scrapers to publishing and earning. This session will showcase the journey of building and monetizing data extraction tools in 4 steps:",General Python knowledge
Transforming REST APIs with Protobuf: Unlocking Performance and Flexibility,"Discover how Protobuf can transform your REST API's schema evolution, while offering performance gains over JSON. This session covers Protobuf's strong versioning, ensuring seamless API updates without breaking clients. We'll tackle the challenges we faced at KAYAK, like the learning curve and integration complexity, offering strategies to address them. Gain practical insights and benchmarks as we discuss integrating Protobuf with Python frameworks, boosting your API's efficiency and adaptability.","In this session we explore how KAYAK adopted Protobuf with REST APIs, replacing the traditional usage of JSON. Designed for API developers, software architects, and Python enthusiasts, this talk will equip you with the knowledge to elevate your API strategies with cutting-edge technology.",Knowledge on REST APIs
FastDjango: Conjuring Powerful APIs with the Sorcery of Django Ninja,"Dive into the world of modern web development by fusing the power of Django and FastAPI. This talk will guide you through the process of building robust, scalable, and efficient APIs using Django Ninja, a web framework that combines Django's reliability and FastAPI's speed. We'll explore how to leverage Django's ORM and user authentication while enjoying FastAPI's performance and type checking. Whether you're a Django veteran looking to supercharge your APIs or a beginner eager to learn cutting-edge techniq","Welcome to 'Django + FastAPI Fusion: Unleashing Robust APIs with Django Ninja Magic', a beginner-friendly exploration into the world of Django Ninja. This talk is designed for those who are just starting their journey in Django and are eager to expand their skills in modern web development.","Python basics, django basics"
Read Your Stocks Via Screenshots,"In this talk, we’ll build a Python app that extracts stock transaction data from screenshots or documents. We’ll refine screenshot extraction accuracy using OCR and regex in an interactive lab environment, store structured data in DuckDB, and visualize insights with Streamlit—transforming raw data into actionable trading insights. This approach is highly adaptable and can be applied to various industries.","In this talk, we’ll build a Python app that extracts stock transaction data from either screenshots or documents. To ensure high OCR accuracy, we‘ll develop an interactive lab environment to visualize every step and dynamically refine regex patterns.",Basic Python skills
Multiplatform testing with Python,"Pareto Security is a suite of native apps that talk with a cloud-based dashboard. We use Python to drive tests across all of these areas, spinning up virtual machines, installing the app, asserting that it works. And to test the cloud-based dashboard, of course.","0–3 min: Introduction to the need for multiplatform testing
3–10 min: Efficient use of pytest for fast feedback, coverage, and parallelization strategies
10–15 min: End-to-end testing with Playwright for sales pages, dashboards, and licensing flows
15–25 min: Testing native Linux and Mac apps with Python automation
25–27 min: Final takeaways, additional resources",Experience with testing
Investing: Technical Analysis libraries in Python,"We will explore the landscape of technical analysis libraries available for the Python language, including popular choices like TA-Lib (aka talib), Pandas TA, and Technical Analysis (aka bukosabino/ta) library.","Do you have financial savings to invest?! In this 55-minute workshop, we'll explore the landscape of technical analysis libraries available for Python, including popular choices like TA-Lib (aka talib), Pandas TA, and Technical Analysis (aka bukosabino/ta) library. We'll explore their capabilities, comparing their pros and cons. Join us to unlock the power of Python in mastering technical analysis!",There are no prerequisites for attending this workshop; everyone is welcome regardless of prior knowledge.
Do Repeat Yourself,Programming is for a large about removing repetition and finding abstractions that achieve that. But is that always sensible? Using the power of music we will examine how universal DRY really is.,"Don't Repeat Yourself (or DRY) is common advice for programmers. While this can be useful in some cases, following this dogmatically can put you in a bad spot. This talk dives into the painful lessons that can arise from this approach, how it can complicate code and increase cognitive load.
Beyond that, we also examine how repetition can facilitate both clarity and learnability and how it can be less evil than it is sometimes made out to be. We'll examine all this through the lens of music programming in Python.",Very basic knowledge of python
Let the Robots Test: Acceptance Test-Driven Development (ATDD) with Robot Framework,"Business specifications are often vague or incomplete, making development challenging. Acceptance Test-Driven Development (ATDD) bridges this gap with clear, executable specifications. This talk explores how Robot Framework enhances collaboration between business and development teams. Through practical examples, we’ll show how to write effective tests and extend Robot Framework with custom Python libraries. Gain insights and tools to improve communication, development, and software delivery.","Developers often face the challenge of working with unclear, ambiguous, or abstract requirements from business stakeholders, leading to misaligned expectations, wasted effort, and costly rework. Without a shared understanding, delivering the right product becomes difficult, resulting in frustration and delays.","Testing, BDD, ATDD"
Oxylabs sponsored talk,Placeholder,,placeholder
Migrating billions records from SQL to NoSQL using continuous migration technique with PySpark and DataProc.,"The batch mechanism is challenging when handling continuous data migration with DataProc. However, I'm introducing a new approach for continuous data pipelines enabled by PySpark. The participants will learn new methods to handle data consistency and reserve data completeness in a million-scale migration from SQL database into NoSQL, MongoDB.","In this talk, I'll present the challenging journey in the real world from my real-world use cases to migrate millions of rows of data from SQL database into NoSQL, MongoDB.","Python in beginner level, Cloud usage experience in beginner level."
Inside the Black Box: The Anatomy of Virtual Environments,"Virtual environments are a fundamental part of Python development, but to most developers, they’re largely a ‘black box’. In this talk, we’re gonna dissect the code, file structure and utilities that make them up to deeply learn, and not just have superficial knowledge of, how venvs actually work.","We all use virtual environments, but do we know how they work? What’s inside a virtual environment? Why do we even have to ‘activate’ them anyways, and what does that mean in the first place? In this talk, we’re gonna discover that.",No previous skills or specific knowledge is required to enjoy the talk.
What We Can Learn from Exemplary Python Documentation,"Let us build on examples from NumPy, pandas, and Matplotlib to explore techniques and tools with the Sphinx documentation generator. Learn how to implement styles, include advanced elements, and overcome challenges in creating clear, maintainable docs. 📑✨","If you’ve attended one of last year’s Python conferences in Europe, you might have seen my talk “Documenting Python Code”, where I introduced attendees to the basics of Python documentation. This year, I will expand on that foundation by looking at what can be learned from exemplary Python documentation.","Set up Sphinx documentation generator, which I can provide in written form beforehand."
Python on the Pitch: How Germany will win World Cup 2026,"We will dive into the fascinating world of football analytics, showcasing how to collect and process match data (e.g., Hudl Statsbomb, Sportmonks, and Understat), including player tracking, event logs, and tactical formations. Attendees will walk away with practical knowledge and Jupyter Notebooks, demonstrating Python's power in decoding modern football strategies.","In this talk, we will explore how Python can be leveraged to analyze and visualize football data for theGermany national football team, managed byJulian Nagelsmann, on their journey to winning the 2026 FIFA World Cup. We will dive into the fascinating world offootball analytics, showcasing how to collect and process match data (e.g., Hudl Statsbomb, Sportmonks, and Understat), including player tracking, event logs, and tactical formations. We'll discover match data to demonstrate how the team's performance reflects Nagelsmann's tactical principles, such asgegenpressing, offensive play, and compactness. Join us to unlock the power of Python in football analytics!","Everyone is welcome regardless of prior knowledge. Real lovers of football are especially encouraged to join, and for fans of the Germany national football team, participation is practically obligatory!"
Automate Brag Document Writing with LLMs,"A brag document is a powerful tool to highlight your work by making it visible, measurable, and demonstrating its real impact on you and your organisation - but such a document can be time-consuming to maintain. My talk explores automation of the writing process with language models fed  with data from tools like Jira, Notion, and code commits. Learn how to save time, avoid registering missed achievements, and make your work stand out. Ideal for engineers at all levels looking to grow their impact.","Getting your work noticed is often the key to expanding your impact and moving forward in your career. It’s not just about doing great work—it’s about making sure it’s being seen and recognized. I recognised this first hand in my previous roles, both as a manager but also as an individual contributor.",No prior knowledge required
Working for a Faster World: Accelerating Data Science with Less Resources,"In data science, speed matters as much as accuracy, especially when users expect quick results. This talk explores simple yet effective techniques to boost performance, using a real-life case of accelerating a Panel app. While some strategies are case-specific, most apply broadly to data-driven projects.","Performance is critical in data science—accuracy alone isn’t enough if applications are slow. Users expect both correct and fast results, and delays can lead to frustration, decreased productivity, and reduced trust in the tools. Whether in web apps, dashboards, or data pipelines, efficient processing is essential for user satisfaction and business success.","Pandas, Data Science, Panel"
Unlocking Probability Distributions with Python,"In this hands-on session, we'll explore the world of probability distributions using Python. From Bernoulli to Gaussian, we'll demonstrate how to apply these distributions to solve real-world problems. Attendees will learn how to use popular Python libraries like NumPy, SciPy, and Matplotlib to visualize and calculate probabilities.","Throughout the session, I'll provide real-world examples of probability distributions in action, highlighting how they're used in finance, healthcare, and other industries. By the end of the session, attendees will have a solid understanding of how to apply probability distributions using Python and will be equipped with the skills to tackle a wide range of problems. We'll conclude by providing resources for further learning and practice, ensuring that attendees can continue to build on their newfound knowledge.",Anaconda/Jupyter Notebook
Accelerating privacy-enhancing data processing,Our mission is simple but profound: to improve and extend lives by learning from the experience of every person with cancer. This talk explains how we transform sensitive data from heterogeneous environments into research-grade datasets. And how we shift insights generation left to iterate faster.,"In this talk, we’ll begin by introducing the concept of real-world evidence datasets and their transformative impact on cancer research. We’ll explore the significant challenges of building high-quality real-world evidence datasets, including the fragmented healthcare data landscape, the complexity of source data, and stringent regulatory constraints.","We expect participants to have some experience with data warehouses or lakehouses, as well as familiarity with Python for data processing."
From Chaos to Control: Automating BI Tools with Pydantic and Python,"Maintaining Business Intelligent Tool (BI) governance, managing permissions, syncing documentation, and handling schema changes, can be chaotic. This talk explores how Python, Pydantic, and smart design patterns automate these tasks, ensuring seamless BI tool governance. Learn how to auto-sync table metadata, adjust queries on column renames, and enforce permissions effortlessly. With real-world examples, discover how to transform BI maintenance from a headache into a streamlined, automated process.","Managing Business Intelligence (BI) tools at scale can quickly become chaotic. Permissions must be enforced, documentation must stay up to date, and queries must be maintained, especially when schema changes occur. Without automation, these tasks become tedious, error-prone, and time-consuming.",Basic Python
Variable Selection: What your model can't tell you,"Variable selection is often left up to an algorithm. However, controlling for some variables can improve measurement accuracy, and thus overall performance. On the other hand, certain ""bad"" controls can block pathways of relationships between variables that we want to preserve or create spurious correlations. Using real and simulated data, I explain when to reconsider your controls, and why that may significantly improve model accuracy.","Econometricians spend a lot of time thinking about causality, whereas data science generally focuses more on prediction and classification. But is there something to be learned from economists' fixation on causal relationships?",Basic linear regression
Beyond dbt: Modern SQL Transformation and Lineage with sqlglot and sqlmesh,"Hear more about the evolving landscape of SQL transformation tools and data lineage challenges. Explore how sqlglot enables powerful SQL parsing and transformation capabilities, and see practical demonstrations of sqlmesh as a modern alternative to dbt. Learn about open-source approaches to data lineage tracking and discover how these tools are shaping the future of data engineering workflows.","This talk explores the intersection of SQL transformation frameworks and data lineage tracking, focusing on open-source solutions that are changing how we handle data transformations at scale. We'll begin by examining common pain points in data lineage tracking, particularly when dealing with complex SQL transformations across different dialects and platforms.","SQL, Python"
Real-time visualization using dash and plotly,"In this paper, a simple live dashboard will be developed using plotly and dash on a practical dataset. This will ease the presentation of data job by abstracting the technicalities and codes from the non-data persons.","Attendees will learn how to create responsive dashboards using Dash, with an emphasis on real-time data processing, integrating live data streams, and efficiently visualizing large datasets. Areas to be discussed are:",intermediate
The Power of Python for Data Management (or How You’ve Been Doing Data Management All Along Without Even Realizing It),Are you using Airflow or Pandas? Great! You've contributed to better data management at your organization.,Are you using Airflow or Pandas? Great! You've been contributing to better data management at your organization.,"No specific skills, knowledge or experience skills required."
"Unlocking Web data with TLSNotary, zkProofs  and LLM while preserving privacy",Imagine sharing the data with a third party without revealing any information while still proving you own the data.,The talk is about how to query web servers and share the data with another party in a secure and privacy preserving way. We will showcase the tlsnotary protocol along with it's plugin generation system powered by LLM (autogen framework Pyautogen).,"LLM , tls protocl , some basic cryptography"
Build & Deploy Apps like a (pro) Data Scientist using Streamlit,Do you ever find it complicated to learn the complexities of a traditional web framework to push your data science work online? Worry no more! Streamlit might help speed things up as it is designed for the required purpose - creating beautiful data-related web apps that can be deployed in minutes.,"0:01-0:05 minutes: In the first section, I will discuss with you the basics of Streamlit and some examples of applications made through it. I will also show you the expected final version of what we’ll create during the tutorial.","Basic understanding of HTML, Python, and libraries such as Numpy, Matplotlib, and Pandas should be good."
Using feature stores to deliver awesome models,"In today’s fast-paced machine learning environment, the ability to efficiently manage and reuse features across multiple models is crucial. This workshop explores how leveraging a feature store can streamline ML pipelines by ensuring consistency and accelerating deployment cycles.
Participants will gain hands-on experience with setting up, managing, and integrating feature stores into their existing workflows—transforming raw data into valuable, production-ready features.","Join us for an immersive, hands-on session designed for data scientists, ML engineers, and AI enthusiasts eager to optimize their machine learning pipelines through advanced feature store functionality. In this workshop, we will focus on the robust capabilities of an open-source feature store platform Feast that streamlines the entire feature lifecycle without requiring you to reinvent the wheel.","Experience in model training, basic SQL knowledge, basic K8s knowledge"
Orchestrating an end-to-end Data Engineering Workflow:  Leveraging Python in Apache Beam and Airflow,"This talk explores the synergy between Apache Beam and Apache Airflow, demonstrating how to create a robust, end-to-end data engineering workflow. We'll dive into the challenges of orchestrating complex data processing tasks and show how combining Airflow's scheduling capabilities with Beam's data processing framework can create more efficient and manageable data pipelines. The session will cover integration with Google Cloud Platform services, including Cloud Functions, BigQuery, and Gemini AI models.","Problem Addressed:
In today's data-driven world, organizations face the daunting challenge of orchestrating complex, end-to-end data engineering workflows that seamlessly integrate batch and streaming processing, scheduling, cloud services, and AI models. This talk tackles the often-overlooked synergy between Apache Beam and Apache Airflow, two powerful tools in the data engineering ecosystem that are rarely used in tandem. We'll explore how combining these technologies with Google Cloud Platform services and cutting-edge AI models can revolutionize data pipeline architecture.",Basic Python and SQL Knowledge
Data Warehouses Meet Data Lakes,"Many organizations have migrated their data warehouses to datalake solutions in recent years.
With the convergence of the data warehouse and the data lake, a new data management paradigm has emerged that combines the best of 2 approaches: the botton-up of big data and the top-down of a classic data warehouse.","In this talk, I will explain the current challenges of a datalake and how we can approach a
moderm data architecture with the help of pyspark, hudi, delta.io or iceberg.
We will see how organize data in a data lake to support real-time processing of applications
and analyzes across all varieties of data sets, structured and unstructured, how provides
the scale needed to support enterprise-wide digital transformation and creates one unique source of data
for multiple audiences.",Nothing
cluster-experiments: A Python library for end-to-end A/B testing workflows,"In this talk, we introduce cluster-experiments, a Python library designed to facilitate end-to-end A/B testing workflows, including power analysis, experiment analysis, and variance reduction techniques.","We will go through the main techniques for mde analysis (simulation based and using Central limit theorem), how to do variance reduction in mde analysis and how to analyse experiments with the same library.","python, basic stats knowledge"
Cutting the price of Scraping Cloud Costs,"A case study of rewriting a simple data pipeline involving Python, a pinch of Go, Git workflows, Airflow, Postgres and Cloud. Investigating some common assumptions and principles of designing data pipelines.
The benefits and issues with the tools and how these may be handled.
I hope this case study of a pipeline rewrite will give you insights that are applicable to Python use for your own data pipelines, and into cloud pricing.","Get answers to the following options and more, as to what is the cheapest and most maintainable solution for this kind of data pipeline.","Python, some exposure to cloud. Some basics of databases and data pipeline may be useful."
"Smarter Retrieval, Better Generation: Improving RAG Systems","Good retrieval performance is key to an effective RAG system, as it ensures relevant information is selected, directly impacting augmentation and generation quality. My presentation focuses on RAG indexing and retrieval, exploring methods to convert text into searchable formats, comparing techniques, and analyzing their advantages, disadvantages, and performance on an annotated dataset to enhance document retrieval based on user queries.","Retrieval Augmented Generation (RAG) is a model architecture for tasks requiring information retrieval from large corpora combined with generative models to fulfill a user information need. It's typically used for question-answering, fact-checking, summarization, and information discovery.",Simply knowing that any information system needs to first index and then retrieve documents should be enough.
Temporal: Bulletproof Workflows,"Temporal is an open source, distributed, and scalable workflow orchestration platform designed to execute mission-critical business logic with resilience. Manage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail.","Are you ready for the next generation of orchestration platforms? Temporal is the successor to Uber Cadence. It is an open source, distributed, and scalable workflow orchestration platform to execute mission-critical business logic with resilience. Your code will run reliably even if it encounters problems, such as network outages or server crashes. With great Developer Experience (DX) and Python SDK, the platform simplifies coding. Join us to unlock the power of Python and Temporal in mastering workflow orchestration!",There are no prerequisites for attending this talk; everyone is welcome regardless of prior knowledge.
Image deduplication using embeddings,"This presentation examines approaches for detecting and eliminating near-duplicate images across datasets ranging from small collections to repositories containing millions of images. We will compare the performance of several embedding models, including CLIP, ResNet, and other variants, assessing their ability to capture semantic and perceptual similarity and performance tradeoffs. We will benchmark various vector database solutions on query speed, memory consumption, and scalability. We will demonstrate p",,General ML and database understanding might be helpfull
Data-Driven Impact in Africa,"There are a lot of NGOs in Africa, trying to help improve lives. The problem is we do not have enough data to help them understand us well to curate impactful humanitarian programs.","Explore how data science can revolutionize NGO work in Africa, focusing on ethical data handling, machine learning applications, and community impact.","python basics, understanding data structures and analysis"
"AI Agents and Digital Trust, The Utilisation of Python In Enhancing Safety In The African Health Care  System","AI agents and digital trust are revolutionizing African healthcare via Python-powered innovations. Advanced machine learning models, built with robust Python libraries, enhance diagnostic precision, predictive analytics, and medical imaging analysis while bolstering cybersecurity and enabling telemedicine. Strict adherence to data privacy, transparency, and ethical standards is crucial to building trust, overcoming infrastructure challenges, and driving sustainable improvements in patient safety and care.","Artificial Intelligence (AI) agents and digital trust are pivotal in transforming healthcare delivery across Africa, where challenges such as limited infrastructure, scarce resources, and high patient loads demand innovative solutions. Python, renowned for its simplicity and extensive libraries, has become a cornerstone in developing AI-driven applications that enhance diagnostic accuracy, enable predictive analytics, and improve medical imaging analysis. These advanced systems also fortify cybersecurity measures, ensuring that sensitive patient data remains secure. Telemedicine platforms and AI chatbots extend medical services to remote and underserved communities, bridging gaps in access to quality care. Moreover, AI-driven decision support systems streamline clinical workflows and reduce human error, empowering healthcare professionals to make informed treatment decisions. Building digital trust is essential; strict adherence to data privacy, transparency, and ethical standards underpins every technological advancement. Collaborative efforts among governments, academic institutions, and private stakeholders are crucial for investing in AI education and infrastructure development. This comprehensive approach not only addresses immediate healthcare challenges but also lays the foundation for a resilient, future-ready system that can serve as a global model for responsible AI deployment in medicine. By continuously embracing innovation and maintaining rigorous standards, Africa’s healthcare ecosystem can achieve sustainable improvements in patient safety, service quality, and equitable access to care. Furthermore, the integration of AI in healthcare is not merely a technological upgrade but a transformative process that redefines patient engagement and clinical decision-making. The use of Python accelerates the development cycle, enabling rapid prototyping and deployment of scalable solutions tailored to the unique challenges of African health systems. Leveraging real-time data analytics a","The knowledge that will be brought to this talk will be about the African health ecosystem,  the infrastructural deficit,  the humongous opportunities abundant in the continent and how to navigate the terrain using data, this talk will be embedded in pure research."
Real-Time Data Analytics at Scale: From Ingestion to Retrieval,"Real-time data analytics is essential for powering modern applications like monitoring, personalization, search, and to some extend, RAG pipelines. However, building systems that can handle real-time ingestion, indexing, and retrieval at scale is no trivial task. This talk provides actionable insights into designing and maintaining such systems at scale using best practices.","Real-time data analytics systems are the backbone of modern applications, from monitoring and personalization to powering search experiences. However, building scalable systems that handle real-time ingestion, indexing, and retrieval efficiently can be daunting. This talk will explore the key components of such systems, including ingestion pipelines (Apache Kafka, Apache Pulsar etc), indexing layers (Elasticsearch, OpenSearch, FAISS, etc), and computation engines (Apache Flink, Apache Spark, custom Python-based solutions...). With the arrival of LLMs and Retrieval Augmented Generation (RAG), data indexation and retrieval become more critical than ever. We will address complex scenarios such asdisaster recovery, data reconciliation, and ensuring low-latency performance at scale. Attendees will leave with actionable insights and best practices to design, implement, and maintain robust real-time analytics systems tailored to their needs.",basic of data engneering/analytics
FastAPI and LamaIndex: A Powerful Combination for AI-Powered Web Development,Take your web apps to the next level by combining FastAPI's speed and efficiency with LamaIndex's AI-driven search engine. This talk will show you how to create powerful search-as-a-service APIs. Unlock new possibilities for your users with advanced search capabilities.,"Ready to supercharge your search capabilities? In today's data-driven world, delivering relevant search results quickly is crucial for a great user experience. But how can you harness the power of AI to make it happen? That's where FastAPI and LamaIndex come in - a dynamic duo that makes it easy to build AI-powered search engines.",Python basics
When safety is non-negotiable - 3 stages of building safety using data & AI,"The more users your platform attracts, the more unwanted attention you'll get from people looking to game your system. While every product is unique, the journey of tackling these bad actors tends to follow similar patterns across companies. In this talk, I'll walk you through the three stages of platform protection that I've witnessed firsthand, and how to level up your safety game using the data you have.","Building effective platform safety isn't a sprint—it's an evolution. Drawing from real-world experience, this session breaks down the three critical phases of safety maturity: from reactive monitoring (""crawl""), to proactive protection (""walk""), to sophisticated, AI-powered prevention (""run""). We'll examine what separates each stage and the exact capabilities needed to advance your safety operations. You'll discover practical approaches to transform your existing data into powerful AI-driven safety tools that shield users from harm while preserving their experience.","None, the topic is more introductory on how to utilise data & AI in safety"
AI Coding Agents and how to code them,"AI Agents are the next big thing everyone has been talking about. They are expected to revolutionize various industries by automating routine tasks, mission critical business workflows, enhancing productivity, and enabling humans to focus on creative and strategic work. Of course, you can apply them to your everyday coding tasks as well.","In this talk we’ll go over what those agents can bring to the table of coding world, and why they can deliver the promise of coding smarter that the current generation of coding assistants can’t. We will then dive right into a quick live coding session where I’ll show what such agents can do in real life and how you can start using them to enhance your everyday life already right after the talk. And we’ll finish off with some remarks on what the future of programming might look like in the near future as those agents get included into your everyday life.",Knowledge about LLM and GenAI
Leveraging Large Language Models for Automated Generation and Validation of Financial Descriptions for Lithuanian Companies,"Scoris.lt utilized Large Language Models (LLMs) to address the challenge of improving SEO performance by generating financial descriptions for Lithuanian companies. The case study highlights the innovative application of LLMs and custom translation models to create high-quality, multilingual content at scale.","The session would outline the implementation of a scalable AI solution by Scoris.lt. Facing challenges with Google search rankings due to a lack of text-based content, Scoris employed LLMs to generate financial descriptions in Lithuanian. The solution combined open-source models for cost efficiency and high throughput, supported by fine-tuned translation models for linguistic accuracy.",None
Retrieval-Augmented Generation for Culturally Nuanced Emotion Analysis in Nigerian Afrobeats Lyrics,"This talk presents a retrieval-augmented generation system tailored for emotion analysis in Nigerian Afrobeats lyrics. Integrating supplementary dictionaries with a hierarchical classifier can enable the system retrieve culturally relevant context to refine emotion predictions. We discuss the architecture, implementation challenges and potential for addressing code-mixing and linguistic nuances in low-resource settings.","In this talk, we introduce a novel retrieval-augmented generation (RAG) system designed to enhance emotion analysis in Nigerian Afrobeats lyrics. Our approach combines supplementary linguistic resources including Igbo-English, Yoruba-English, Pidgin-English, and English-Hausa dictionaries, with a hierarchical classifier. The system first uses a root-level classifier to assign a meta-emotion and then retrieves contextually relevant passages from a combined corpus of lyrics and dictionary entries. This retrieved context is appended to the original input, allowing the branch-level classifier to make more informed fine-grained predictions.","AI, LLMS, Music"
Unlocking the Power of Python and PyTorch for Biomedical Image Segmentation,"How can machine learning enhance biomedical image analysis? This talk explores the potential of Python and PyTorch in automating artifact and damage segmentation. From data preprocessing to clustering-based label classification and deep learning-driven segmentation, key techniques will be discussed, including the use of Convolutional Neural Network architectures. The session will also cover performance evaluation and insights into advancing biomedical imaging with AI-driven solutions.","The field of pathology is undergoing rapid digitalization, integrating new technologies that optimize workflows, reduce manual effort, and enhance diagnostic accuracy. However, preprocessing remains a labor-intensive step, requiring specialists to manually segment out artifacts before further analysis can take place. This talk explores how Python and PyTorch can be leveraged to automate this crucial preprocessing stage using machine learning techniques.",Machine Learning basics (preferably Biomedical field)
"📂 Slow Productivity AI: Automating Knowledge & Task Management with Offline Hugging Face, n8n & Obsidian","Modern work demands constant context-switching—emails, notes, meetings, and tasks pile up, leaving us overwhelmed. This talk introduces a<b>slow productivity AI</b>approach, inspired by<b>Cal Newport</b>, that leverages<b>offline, open-source automation</b>using<b>Hugging Face, n8n, and Obsidian</b>. By structuring knowledge into meaningful tasks<b>without disrupting deep work</b>, we can create a<b>sustainable, low-distraction workflow</b>—working smarter, not just faster.","In his book Slow Productivity, Cal Newport argues that modern work culture prioritizes busyness over effectiveness, leading to stress, shallow work, and burnout. But what if AI could enhance deep work rather than create more digital noise?",Basic Python knowledge
Vectors Everywhere: An Introduction to Vector Databases,"You have probably heard of vector databases while learning about embeddings used in deep learning models or while exploring Retrieval Augmented Generation(RAG).
In that case, you might be wondering why not just use SQL databases or NoSQL databases.
In this talk you will learn what embeddings are, and what vector databases are, why they are useful and when to use them. Expect to understand why vector databases are preferred for certain applications over SQL and NoSQL databases.","Vector databases have become more commonplace with the adoption of LLMs in various systems. You may have heard of vector databases in passing and are wondering what they are and why they are all the rage despite SQL databases having stood the test of time. You might remember learning about vectors in school and are wondering what's their relation to LLMs.
In this talk, we will learn about vectors, embeddings and vector databases. You will form an understanding of when vector databases are applicable and work through how to use open source vector databases.
This session is perfect for Python developers, data engineers and ML engineers curious about data storage and retrieval. Join us to learn the basics of vector databases and how they are powering some of the world's AI systems.",Python and SQL databases.
Build Your Own Observability Tool from Scratch for AI Agents,"In this hands-on workshop, we will learn how to develop a custom observability solution tailored for AI agents. From logs and metrics to guardrail checks and cost tracking, we’ll discover how to monitor crucial elements like accuracy, latency, and environmental impact. Walk away with your own tool and best practices to ensure AI agents are both well-observed and well-behaved.","AI agents can exhibit complex behaviors, including inconsistent accuracy, and unexpected spikes in resource usage. Traditional monitoring tools often fail to capture these nuances effectively. In this workshop, we will build an observability tool from scratch using Python, integrating techniques like advanced logging, metrics tracking, guardrail checks, and real-time dashboards. We will also address carbon emissions and cost visibility, highlighting the importance of responsible AI development. By the end, you’ll have a solid understanding of how to create a fully customizable solution that scales and adapts to your AI workflows.","Basics of AI Agents, Python and related tools"
The Emergence of Agentic Workflows in AI,"AI is evolving from passive tools to autonomous agents, driving the rise of agentic workflows that can plan, execute, and optimize tasks with minimal human input. This session will explore how large language models and multi-agent collaboration power these systems, enhancing efficiency and innovation. Attendees will gain insights into real-world applications, challenges, and the future of autonomous AI systems, uncovering how agentic workflows are transforming industries.","As AI systems transition from passive tools to autonomous agents, agentic workflows are emerging as a new paradigm. These AI-driven systems autonomously plan, execute, and optimize tasks with minimal human input, transforming areas like automation and decision-making. This session will focus on how developers can leverage large language models and multi-agent collaboration to build efficient, innovative agentic workflows. Code examples using a popular agentic framework will be demonstrated, providing hands-on insights into real-world applications. Attendees will also explore challenges and future directions for autonomous AI systems, gaining valuable knowledge on how agentic workflows are accelerating AI-driven change.",Familiarity with LLMs
I messed up so you don't: Surprisal and the headache of tokenizer encodings in LLMs!,"What can go wrong with tokenizer encodings? Everything! I will share my experience of understanding, misunderstanding, and ultimately learning to work with tokenization in LLMs. I will discuss what surprisal is, its relevance to my research, and its connection to tokenization. The talk will include various examples illustrating how misunderstandings of tokenization can arise, as well as strategies for debugging and preventing these issues.",,"Basic Python, and interest in NLP/LLMs"
A Crash course in Time Series Forecasting from Naive to Foundational,Forecasting is a common activity that has clear business value in various domains but it is not a very common skill that Data Scientists have or feel confident about. In this crash course I will cover the fundamentals of Time Series forecasting from the basic methods to more advanced techniques. I will do this showcasing practical code examples using libraries from Nixtla.,This is a technical talk that aims to provide the essential elements for a Data Scientist to go from zero knowledge of time series forecasting to being able to code a forecasting system that can evolve from a basic setup that can be put in production.,Basic Python skills. A background in Data Science is beneficial
“Beginner's Guide to building Autonomous AI Agents with Eliza Framework”,"AI agents are becoming fundamental building blocks of modern applications, thanks to their capability of understanding context with the help of the LLMs, but with added capability to interact with systems, files. AI agents that can meaningfully interact across platforms while maintaining context aren't just for large tech companies anymore as we have a number of open source AI Agent frameworks now which developers can leverage. This beginner-friendly, hands-on tutorial shows the process of creating autonomo","We'll break down the complexities of AI agent development into understandable components. Starting with basic concepts and progressing to functional implementations, participants will learn how to create, deploy, and manage AI agents that can handle real-world tasks with the help of the Eliza AI Agent framework. Finally we will have a hands on demo","basics of python, vector search, llm"
"How We Outperformed Microsoft, Google, and OpenAI in Speech-to-Text","We built a cutting-edge speech-to-text model that outperformed solutions from industry leaders like Microsoft, Google, and OpenAI. This is the story of how we identified a market gap, defined what makes ""the best model,"" and turned our vision into a successful business.","In this talk, I’ll share my personal journey of how we started our own business and built an innovative audio (speech-to-text) model that outperformed the offerings of industry giants like Microsoft, Google, and OpenAI. I’ll discuss how we identified a crucial gap in the market, carried out in-depth analysis to define what makes ""the best model,"" and overcame the challenges of competing with established players. This story is about determination, creativity, and the power of a focused vision, showing that even small teams can achieve big results when they find the right opportunities.",curiosity
The Best of Both Worlds: A Hybrid Approach to Lightning-Fast Product Matching,"In an era dominated by data, businesses struggle with processing diverse, unstructured information across systems. This research presents an AI-powered pipeline addressing product matching challenges in retail and e-commerce. Our solution combines traditional matching algorithms with deep learning through a five-step process. This approach minimizes manual intervention while improving accuracy and efficiency.","In today's data-driven world, businesses face the challenge of managing vast amounts of information, often from diverse and unstructured sources. This complexity necessitates efficient and accurate data processing techniques to extract meaningful insights and drive informed decision-making. One such critical task is product matching, where the goal is to accurately identify and link records representing the same product across different systems or datasets. This is particularly crucial for businesses operating in industries with complex product catalogs, such as retail, manufacturing, and e-commerce. Our project solves this problem by developing a robust, AI-powered pipeline to automate the matching process while minimizing human intervention.
The task involved matching incoming product records with a standardized catalog, referred to as the ""G_List,"" which includes essential attributes. The primary challenge lay in the inherent variability and inconsistency of product data. Incoming records often included: spelling and grammatical errors, data inconsistencies, multilingual variations. Examples of the challenge include identifying that ""Yellow Glass whiskey 1L,"" ""Color Whiskey 1L glas,"" and ""GLAS amarillo whiskey"" all correspond to ""GLASS YELLOW WHISKEY 1L"" in the G_List.
Our solution comprises five key steps. First, we created the G_List by standardizing product data and attributes. Next, we consolidated source data into a unified data lake, where records were cleaned and transformed. The third step utilized the tfidf_matcher Python library for initial matching. Step four, feature extraction. Finally, a  deep learning model assessed the matches as either ""PASS"" or ""NOT PASS."" Records flagged as ""PASS"" were automatically labeled and finalized, while ""NOT PASS"" records were escalated for manual review.
This hybrid approach of combining matching algorithms with AI significantly reduced manual workload while improving accuracy.",Basic Computer Science knowledge.
Building Compliant and Robust AI Systems: Navigating the EU AI Act with Python,"This session provides a concise overview of EU AI Act compliance tailored for Python developers building AI systems. We'll explore key compliance areas, including risk management, robust model testing, cybersecurity, and explainability, highlighting practical tools and popular Python libraries.","In this talk, you'll gain essential insights into aligning Python-based AI system development with the EU AI Act’s regulatory requirements. We'll begin with a brief introduction to the EU AI Act’s core concepts, risk tiers, and certification expectations, emphasizing what matters most for practical compliance.",Interest in AI system development
,,,
Putting PyTorch Models into Production,"Deploying PyTorch models into production requires balancing performance, hardware efficiency, and cost optimization. Developers face challenges such as GPU memory constraints during training, suboptimal inference latency, and escalating cloud infrastructure expenses. For instance, modern models like LLMs demand specialized strategies to reduce GPU memory footprint while maintaining throughput.","Deploying PyTorch models into production requires balancing performance, hardware efficiency, and cost optimization. Developers face challenges such as GPU memory constraints during training, suboptimal inference latency, and escalating cloud infrastructure expenses. For instance, modern models like LLMs demand specialized strategies to reduce GPU memory footprint while maintaining throughput. Meanwhile, serving architectures often suffer from poor hardware utilization due to rigid microservice designs, leading to unnecessary costs. This session addresses these pain points by providing a roadmap for transitioning PyTorch models from research to production, emphasizing GPU optimization and cost-aware serving.",PyTorch basics
Knowledge Bases & Memory for Agentic AI,"Some of the latest big evolutionary steps in generative AI has been models that support function calling and “agentic” capabilities.  This is provides generative models with “tools” that allow them to go beyond generating outputs for simple queries, and start planning the best way to solve complex queries.  In this talk, we’ll be diving into using vector databases as the backbone for these types of complex AI architectures, both serving as knowledge bases, and memory.","In this technical talk, we will start by covering the history of how agentic AI came about. We will go over how we can design prompts in a way that instruct LLMs to use tools and plan out how to solve complex queries. Next, we will learn about function calling and how this feature of LLMs can be used as the basis of agents.",Basic understanding of vector databases and vector search would be useful.
"Artificial Intelligence in Open-Source Project Management: Automating Tasks, Predicting Outcomes, and Optimizing Resources","As artificial intelligence continues to evolve, there is a growing need for project maintainers to leverage on AI to foster diverse and inclusive collaborations. In this session, I will discuss importance of artificial intelligence in open-source project management, actionable strategies to maximize diverse talents and promote transparency, tools and technologies available for project maintainers, and practical ways to leverage these technologies to make a meaningful impact within open-source communities.","In this session, there will be interactive sessions and case studies to provide practical insights and how to navigate various forms of challenges and opportunities to aid my objectives which include;",No specific skills
"Anonymization of sensitive information in financial documents using, python, diffusion models and named entity recognition","Unlock sensitive data potential with anonymization! Learn how Python, diffusion models, and Named Entity Recognition (NER) empower institutions to anonymize PII in financial documents, replacing it with synthetic stand-ins. Discover open-source, self-hosted tools to ensure privacy while unleashing data's full power.","Anonymization of sensitive information in financial documents using, python, diffusion models and named entity recognition",Python
Enhancing Model Context Protocol (MCP) with Dynamic Tool Discovery for Smarter AI,"The Model Context Protocol (MCP) is an emerging standard that enables structured data provisioning for LLMs and AI agents. However, the current data discovery mechanism in MCP is static.  This limits the AI’s ability to dynamically assess the utility, relevance, and efficiency of data tool calls in real time.  Here I present an enhancement to MCP ""tool discovery"" that introduces dynamic data descriptions, allowing LLM to be better informed.",This talk will introduce the concept of MCP with a demonstration of how MCP clients and servers. How they are built and how they interact.,"Python, LLM"
Code Generation in Regulated Industries: Opportunities and Challenges,"AI-driven code generation can transform software development in regulated sectors like banking and insurance - but only if implemented securely and responsibly. In this talk, we’ll explore how to harness tools like GitHub Copilot and ChatGPT to boost productivity while ensuring compliance. Attendees will learn key considerations, best practices, and practical insights to keep code generation both efficient and fully auditable.","In highly regulated industries, code quality, security, and compliance are paramount. Yet recent advances in AI-driven coding assistants promise faster development, fewer errors, and improved agility. How do we tap into these benefits without introducing unacceptable risk?",None
EGTL data-processing model prototype using Python,"Discover how EGTL (Extract, Generate, Transfer, Load) extends traditional ETL by adding a “generate” step powered by GenAI. In this talk, I’ll demonstrate how Python pipelines on top of data warehouse can automatically extract data, generate new insights, and deliver optimized transformations. We’ll explore practical workflows, real-world use cases, and best practices—equipping you to apply EGTL in your own data projects.","Experience a novel approach to data ingestion pipelines with EGTL (Extract, Generate, Transfer, Load), a practical evolution of the standard ETL process. By incorporating a “generate” step powered by GenAI into the workflow, EGTL unlocks new potential in data transformation, advanced data analytics, and automation. In this talk, I’ll walk you through the core components of EGTL, demonstrating how Python-based data pipelines can leverage generative AI to produce enriched datasets on the fly before transferring and loading them into downstream systems.
Using real-world use cases, I’ll illustrate how EGTL benefits both data engineers and scientists by reducing manual overhead, accelerating iterative development, and unveiling unexpected insights. You’ll learn implementation best practices—covering everything from tool selection and architecture design to error handling and governance. This talk will highlight key strategies for integrating GenAI directly into your pipelines.","data engineering, automation, big data application develpment"
AI 360: From Theory to Transformation,"This talk charts the evolution of Artificial Intelligence through the dual lenses of data and models, tracing AI’s journey from early symbolic systems to today’s advanced data-driven techniques. Attendees will learn how the interplay of ever-growing datasets and increasingly sophisticated model architectures has powered major breakthroughs, transforming AI from theoretical curiosity to a global catalyst for innovation.","We will begin by examining AI’s early days, when handcrafted rules and symbolic reasoning took center stage despite limited data and computational resources. Next, the spotlight shifts to the rise of machine learning and neural networks, as larger datasets and improved computing power enabled more flexible, adaptive models. Along the way, we will highlight pivotal milestones—such as the resurgence of deep learning—that propelled AI forward at an accelerated pace. By focusing on how evolving data availability and model complexity shaped each phase of AI, this session provides a comprehensive historical perspective and offers insights into how AI’s trajectory continues to unfold in today’s data-rich world.",General curiosity in AI
"GenAI for Clients: No pain, no gain",What is the way from prove of concept to big production solutions for GenAI application? How to make it scalable and make one release by 7 sprints?,"We talk about different GenAI Accenture cases in production and preproduction studies, how to implement new technologies and what advantages and disadvantages we can discover during such projects. How often client really know what they want? GenAI ""magic"" tricks or 99% accuracy? Let's talk about LLM, stable diffusion and similarity search.","AI fundamentals, Programming, Azure fundamentals, AWS fundamentals"
